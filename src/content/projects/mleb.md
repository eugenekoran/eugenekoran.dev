---
title: "MultiLingual Exam Benchmark"
description: "Developed a novel evaluation dataset and framework for assessing advanced multilingual capabilities of Large Language Models."
pubDate: 2024-10-01
tags: ["LLM Evaluation", "Python", "Inspect AI"]
---

## Overview

A novel evaluation framework designed to rigorously test the multilingual capabilities of Large Language Models across diverse dimensions including language understanding, domain-specific knowledge, visual reasoning, and instruction following.

## Key Contributions

- Designed comprehensive evaluation framework spanning multiple capability dimensions
- Created evaluation datasets testing multilingual understanding and domain expertise
- Open-sourced the initial dataset and codebase on GitHub

## Links

- [GitHub Repository](https://github.com/eugenekoran/mleb)

## Technologies

Python, OpenAI API, Inspect AI
