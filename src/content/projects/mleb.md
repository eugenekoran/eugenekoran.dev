---
title: "MultiLingual Exam Benchmark"
description: "Developed an evaluation dataset and framework for assessing multilingual capabilities of Large Language Models."
pubDate: 2024-09-15
tags: ["LLM Evaluation", "Python", "Inspect AI"]
---

## Overview

An evaluation framework for testing the multilingual capabilities of Large Language Models across language understanding, domain-specific knowledge, and visual reasoning.

## Key Contributions

- Designed an evaluation framework covering multilingual understanding and domain expertise
- Open-sourced initial dataset and codebase on GitHub

## Links

- [GitHub Repository](https://github.com/eugenekoran/mleb)

## Technologies

Python, OpenAI API, Inspect AI
